{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터 전처리\n",
    "- 필요한 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import koreanize_matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/wanted/ml/env311/preject/data/rawData_utf.csv',encoding='utf-8')\n",
    "df['date'] = df['date'].astype('str')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. 데이터 전처리\n",
    "- 결측치 확인 및 제거\n",
    "    - 강수량(rain) : 0\n",
    "    - 토지 (total_area, field_area, paddy_area, cemetery_area): 동일 지역 전년도 값\n",
    "    - 기온 (tempAvg, tempMin, tempMax): 동일 지역 같은 달의 평균\n",
    "    - 습도 (humMin, humAvg): 동일 지역 같은 달의 평균\n",
    "    - 풍속 (windMax, windAvg): 동일 지역 같은 달의 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{col:df[col].isnull().sum() for col in df.columns[df.isnull().sum() > 0].tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fill_with_average(x):\n",
    "#     return x.apply(lambda col: col.fillna(x[col.name].mean()))\n",
    "\n",
    "filled_df = df.copy()\n",
    "filled_df.insert(0, 'dateYear', filled_df['date'].str[:4])\n",
    "filled_df.insert(1, 'dateMonth', filled_df['date'].str[4:6])\n",
    "filled_df.insert(2, 'dateDay', filled_df['date'].str[6:])\n",
    "\n",
    "filled_df.drop('date', axis=1)\n",
    "# # 1월 ~ 6월까지만 추출\n",
    "filled_df = filled_df[filled_df['dateMonth'].isin(['01', '02', '03', '04', '05', '06'])]\n",
    "filled_df['rain'] = filled_df['rain'].fillna(0)\n",
    "\n",
    "weather_columns = ['tempAvg', 'tempMin', 'tempMax', 'windMax', 'windAvg', 'humMin', 'humAvg']\n",
    "\n",
    "for col in weather_columns:\n",
    "    filled_df[col].fillna(filled_df.groupby(['dateYear', 'dateMonth', 'sgg_nm'])[col].transform('mean'), inplace=True)\n",
    "\n",
    "area_columns = ['total_area', 'field_area', 'paddy_area', 'cemetery_area']\n",
    "for col in area_columns:\n",
    "    filled_df[col] = filled_df.groupby('sgg_nm')[col].transform(lambda group: group.ffill().bfill())\n",
    "\n",
    "data = filled_df.copy()\n",
    "data['farm_ratio'] = (data['paddy_area'] + data['field_area']) / data['total_area']\n",
    "data['cemetary_ratio'] = data['cemetery_area'] / data['total_area']\n",
    "data['population_density'] = data['population'] / data['total_area']\n",
    "data.insert(len(data.columns)-1, 'occur', data.pop('occur'))\n",
    "data = data.drop(columns=['date', 'dateYear', 'dateMonth', 'dateDay', 'sgg_nm', 'field_area', 'paddy_area', 'cemetery_area', 'total_area', 'population'])\n",
    "\n",
    "loc_datas = {do:None for do in data.sd_nm.unique()}\n",
    "\n",
    "for do in loc_datas.keys():\n",
    "    loc_df = data.copy()\n",
    "    loc_df = loc_df[loc_df['sd_nm'] == do]\n",
    "    loc_datas[do] = loc_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 머신러닝 관련 패키지 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import (confusion_matrix, accuracy_score, roc_curve, auc as metrics_auc,\n",
    "                             precision_recall_curve, precision_score, recall_score, roc_auc_score)\n",
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 피쳐엔지니어링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataProcessing_v2(input_data):\n",
    "    \"\"\"\n",
    "    주어진 데이터에서 머신러닝 모델 학습을 위한 전처리(특성 선택, 샘플링, 스케일링)를 수행하는 함수\n",
    "\n",
    "    Args:\n",
    "    - input_data (pd.DataFrame): 원본 데이터셋\n",
    "\n",
    "    Returns:\n",
    "    - X_train_scaled (ndarray): 학습 데이터(입력 변수) - 표준화 적용됨\n",
    "    - Y_train_resampled (ndarray): 학습 데이터(타겟 변수) - 샘플링 적용됨\n",
    "    - X_test_scaled (ndarray): 테스트 데이터(입력 변수) - 표준화 적용됨\n",
    "    - Y_test (ndarray): 테스트 데이터(타겟 변수) - 원본 분포 유지\n",
    "    \"\"\"\n",
    "    \n",
    "    ml_data = input_data.copy()\n",
    "    ml_data_X = ml_data.drop(['sd_nm',\"windMax\",'tempAvg','humAvg','tempMin', 'occur'], axis=1)\n",
    "    ml_data_Y = ml_data['occur']\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(ml_data_X, ml_data_Y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # 샘플링 적용\n",
    "    smote = SMOTE(sampling_strategy=0.5,random_state=42)\n",
    "    under = RandomUnderSampler(sampling_strategy=0.5,random_state=42)\n",
    "    pipeline = Pipeline(steps=[('smote',smote),('under',under)])\n",
    "    X_train_resampled, Y_train_resampled = pipeline.fit_resample(X_train, Y_train)\n",
    "    \n",
    "    # 스케일링 적용\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, Y_train_resampled, X_test_scaled, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 모델 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateModel(model, param_grid, xtrain, ytrain, xtest, ytest):\n",
    "    \"\"\"\n",
    "    모델을 학습시키고 최적 하이퍼파라미터를 찾은 후 평가 및 시각화를 수행하는 함수\n",
    "\n",
    "    Args:\n",
    "    - model: 머신러닝 모델\n",
    "    - param_grid: 하이퍼파라미터 그리드\n",
    "    - xtrain, ytrain: 학습 데이터\n",
    "    - xtest, ytest: 테스트 데이터\n",
    "    - model_name: 모델 이름 (str)\n",
    "\n",
    "    Returns:\n",
    "    - best_model: 최적의 하이퍼파라미터를 가진 모델\n",
    "    \"\"\"\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        model\n",
    "        , param_grid\n",
    "        , cv=5\n",
    "        , scoring=['roc_auc', 'average_precision']\n",
    "        , refit='average_precision'\n",
    "        , n_jobs=-1\n",
    "        # , verbose=1\n",
    "    )\n",
    "    grid_search.fit(xtrain, ytrain)\n",
    "\n",
    "    # 최적 모델 가져오기\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # 2. 예측 수행\n",
    "    Y_pred = best_model.predict(xtest)\n",
    "    Y_pred_proba = best_model.predict_proba(xtest)[:, 1]\n",
    "\n",
    "    # 임계값 찾기\n",
    "    precision,recall,thresholds = precision_recall_curve(ytest,Y_pred_proba)\n",
    "    gmean_scores = []\n",
    "    for thresh in thresholds:  # \n",
    "        Y_pred = (Y_pred_proba >= thresh).astype(int)  \n",
    "        sensitivity = recall_score(ytest, Y_pred)\n",
    "        specificity = recall_score(ytest, Y_pred, pos_label=0)\n",
    "        gmean = np.sqrt(sensitivity * specificity)\n",
    "        gmean_scores.append(gmean)\n",
    "\n",
    "    # 오류 수정: thresholds 배열 참조\n",
    "    best_idx_gmean = np.argmax(gmean_scores)\n",
    "    best_threshold_gmean = thresholds[best_idx_gmean] if best_idx_gmean < len(thresholds) else 0.3\n",
    "\n",
    "    Y_pred_optimal = (Y_pred_proba >= best_threshold_gmean).astype(int)\n",
    "     # 3. 평가 지표 계산\n",
    "    metrics = {\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'accuracy': accuracy_score(ytest, Y_pred_optimal),\n",
    "        'precision': precision_score(ytest, Y_pred_optimal),\n",
    "        'recall': recall_score(ytest, Y_pred_optimal),\n",
    "        'roc_auc': roc_auc_score(ytest, Y_pred_proba),\n",
    "        'confusion_matrix': confusion_matrix(ytest, Y_pred_optimal),\n",
    "        'fpr_vals': roc_curve(ytest, Y_pred_proba)[0],\n",
    "        'tpr_vals': roc_curve(ytest, Y_pred_proba)[1],\n",
    "        'precision_vals': precision,\n",
    "        'recall_vals': recall,\n",
    "        'pr_auc': metrics_auc(recall, precision)\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(X_train,y_train,model,estimators):\n",
    "    '''\n",
    "    여러 개의 머신러닝 모델을 앙상블(Ensemble)하여 최종 예측 성능을 향상시키는 함수.\n",
    "    StackingClassifier를 사용하여 개별 모델들의 예측 결과를 종합한 후, 최종 모델(final_estimator)이 이를 기반으로 학습.\n",
    "    '''\n",
    "    model = StackingClassifier(\n",
    "        estimators=estimators,\n",
    "        final_estimator=model\n",
    "    )\n",
    "\n",
    "    model.fit(X_train,y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 하이퍼파라미터 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params_dict = {\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(),\n",
    "        'params': {\n",
    "            'C': [0.01, 0.1, 1, 10, 100]\n",
    "        },\n",
    "        'color': 'blue'\n",
    "    },\n",
    "    'XGBClassifier': {\n",
    "        'model': XGBClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 300, 500]\n",
    "            , 'max_depth': [3, 6, 9]\n",
    "            , 'learning_rate': [0.01, 0.1, 0.2]\n",
    "            # , 'tree_method': ['gpu_hist']\n",
    "            # , 'gpu_id': [0]\n",
    "        },\n",
    "        'color': 'red'\n",
    "    },\n",
    "    'LGBMClassifier': {\n",
    "        'model': LGBMClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 300, 500]\n",
    "            , 'num_leaves': [31, 63, 127]\n",
    "            , 'learning_rate': [0.01, 0.1, 0.2]\n",
    "            # , 'device': ['gpu']\n",
    "        },\n",
    "        'color': 'purple'\n",
    "    },\n",
    "    'RandomForestClassifier': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 300, 500]\n",
    "            , 'max_depth': [None, 10, 20, 30]\n",
    "            , 'min_samples_leaf': [2, 5, 10]\n",
    "        },\n",
    "        'color': 'green'\n",
    "    }\n",
    "}\n",
    "model_name = [i[\"model\"] for i in model_params_dict.values()]\n",
    "model_params_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 지역별 모델 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지역별 데이터_v2\n",
    "loc_results = dict()\n",
    "\n",
    "for loc, loc_data in loc_datas.items():\n",
    "    tmp_dict = dict()\n",
    "    \n",
    "    x_loc_train, y_loc_train, x_loc_test, y_loc_test = DataProcessing_v2(loc_data)\n",
    "    \n",
    "    for model in model_params_dict.keys():\n",
    "        print(f'{loc} - {model} Start : {datetime.datetime.now()}')\n",
    "        estimators = [(name, model_params_dict[name]['model']) for name in model_params_dict.keys() if name != model]\n",
    "        param_grid = {f'final_estimator__{param}': values for param, values in model_params_dict[model]['params'].items()}\n",
    "\n",
    "        tmp_dict[model] = EvaluateModel(\n",
    "                            model = ensemble(x_loc_train,y_loc_train,model_params_dict[model]['model'],estimators)\n",
    "                            , param_grid = param_grid\n",
    "                            , xtrain = x_loc_train\n",
    "                            , ytrain = y_loc_train\n",
    "                            , xtest = x_loc_test\n",
    "                            , ytest = y_loc_test\n",
    "        )\n",
    "        print(f'{loc} - {model} Finish : {datetime.datetime.now()}')\n",
    "    \n",
    "    loc_results[loc] = tmp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dict.keys()\n",
    "\n",
    "import joblib\n",
    "for key,model in tmp_dict.items():\n",
    "    joblib.dump(model, f\"{key}.pkl\")\n",
    "    print(f\"{key} 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_result_df = pd.DataFrame(\n",
    "    {(region, model): metrics for region, models in loc_results.items() for model, metrics in models.items()}\n",
    ").T\n",
    "\n",
    "region_result_df.index.names = [\"Region\", \"Model\"]\n",
    "region_result_df.to_csv('../region_result.csv')\n",
    "\n",
    "region_result_df.insert(1, 'true_negative', region_result_df['confusion_matrix'].apply(lambda x: x[0][0]))\n",
    "region_result_df.insert(2, 'fale_positive', region_result_df['confusion_matrix'].apply(lambda x: x[0][1]))\n",
    "region_result_df.insert(3, 'fale_negative', region_result_df['confusion_matrix'].apply(lambda x: x[1][0]))\n",
    "region_result_df.insert(4, 'true_positive', region_result_df['confusion_matrix'].apply(lambda x: x[1][1]))\n",
    "\n",
    "filt_region_result_df = region_result_df.copy()\n",
    "filt_region_result_df = filt_region_result_df.drop(['best_params','confusion_matrix'] + [col for col in region_result_df.columns if col.endswith('vals')], axis=1)\n",
    "\n",
    "filt_region_result_df.to_csv('../region_result_filt.csv')\n",
    "filt_region_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 전국 모델 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전국 데이터\n",
    "nat_results = dict()\n",
    "\n",
    "x_train, y_train, x_test, y_test = DataProcessing_v2(data)\n",
    "\n",
    "for model in model_params_dict.keys():\n",
    "    print(f'{model} Start : {datetime.datetime.now()}')\n",
    "    estimators = [(name, model_params_dict[name]['model']) for name in model_params_dict.keys() if name != model]\n",
    "    param_grid = {f'final_estimator__{param}': values for param, values in model_params_dict[model]['params'].items()}\n",
    "    \n",
    "    nat_results[model] = EvaluateModel(\n",
    "                        model = ensemble(x_train,y_train,model_params_dict[model]['model'],estimators)\n",
    "                        , param_grid = param_grid\n",
    "                        , xtrain = x_train\n",
    "                        , ytrain = y_train\n",
    "                        , xtest = x_test\n",
    "                        , ytest = y_test\n",
    "    )\n",
    "    print(f'{model} Finish : {datetime.datetime.now()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nat_results.keys()\n",
    "\n",
    "import joblib\n",
    "for key,model in nat_results.items():\n",
    "    joblib.dump(model, f\"korea_{key}.pkl\")\n",
    "    print(f\"전국{key} 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nation_result_df = pd.DataFrame(\n",
    "    {model: metrics for model, metrics in nat_results.items()}\n",
    ").T\n",
    "\n",
    "nation_result_df.index.names = [\"Model\"]\n",
    "nation_result_df\n",
    "# nation_result_df.to_csv('../result/nation_result.csv')\n",
    "\n",
    "nation_result_df.insert(1, 'true_negative', nation_result_df['confusion_matrix'].apply(lambda x: x[0][0]))\n",
    "nation_result_df.insert(2, 'fale_positive', nation_result_df['confusion_matrix'].apply(lambda x: x[0][1]))\n",
    "nation_result_df.insert(3, 'fale_negative', nation_result_df['confusion_matrix'].apply(lambda x: x[1][0]))\n",
    "nation_result_df.insert(4, 'true_positive', nation_result_df['confusion_matrix'].apply(lambda x: x[1][1]))\n",
    "\n",
    "nation_result_df = nation_result_df.copy()\n",
    "filt_nation_result_df = nation_result_df.drop(['best_params','confusion_matrix'] + [col for col in nation_result_df.columns if col.endswith('vals')], axis=1)\n",
    "\n",
    "# filt_nation_result_df.to_csv('../result/nation_result_filt.csv')\n",
    "filt_nation_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 평가 결과 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 지역별 모델 평가 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for idx, (r, m) in enumerate(region_result_df.index):\n",
    "    if idx % 4 == 0:\n",
    "        plt.subplot(2,4,(idx//4 +1))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    plt.plot(\n",
    "        region_result_df.loc[(r, m), 'fpr_vals']\n",
    "        , region_result_df.loc[(r, m), 'tpr_vals']\n",
    "        , color=model_params_dict[m]['color']\n",
    "        , lw=2\n",
    "        , label=f\"{m} (AUC = {region_result_df.loc[(r, m), 'roc_auc']:.2f})\"\n",
    "    )\n",
    "            \n",
    "    plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "    plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "    plt.title(f\"ROC Curve ({r})\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PR-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "for idx, (r, m) in enumerate(region_result_df.index):\n",
    "    if idx % 4 == 0:\n",
    "        plt.subplot(2,4,(idx//4 +1))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    plt.plot(\n",
    "        region_result_df.loc[(r, m), 'recall_vals']\n",
    "        , region_result_df.loc[(r, m), 'precision_vals']\n",
    "        , color=model_params_dict[m]['color']\n",
    "        , lw=2\n",
    "        , label=f\"{m} (AUC = {region_result_df.loc[(r, m), 'pr_auc']:.2f})\"\n",
    "    )\n",
    "            \n",
    "    plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"Precision-Recall Curve ({r})\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전국 모델 평가 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "for m in nation_result_df.index:\n",
    "    plt.plot(\n",
    "        nation_result_df.loc[m, 'fpr_vals']\n",
    "        , nation_result_df.loc[m, 'tpr_vals']\n",
    "        , color=model_params_dict[m]['color']\n",
    "        , lw=2\n",
    "        , label=f\"{m} (AUC = {nation_result_df.loc[m, 'roc_auc']:.2f})\"\n",
    "    )\n",
    "            \n",
    "plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate (FPR)\")\n",
    "plt.ylabel(\"True Positive Rate (TPR)\")\n",
    "plt.title(f\"ROC Curve (전국)\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "for m in nation_result_df.index:\n",
    "    plt.plot(\n",
    "        nation_result_df.loc[m, 'recall_vals']\n",
    "        , nation_result_df.loc[m, 'precision_vals']\n",
    "        , color=model_params_dict[m]['color']\n",
    "        , lw=2\n",
    "        , label=f\"{m} (AUC = {nation_result_df.loc[m, 'pr_auc']:.2f})\"\n",
    "    )\n",
    "        \n",
    "plt.plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(f\"Precision-Recall Curve (전국)\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env311_cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
